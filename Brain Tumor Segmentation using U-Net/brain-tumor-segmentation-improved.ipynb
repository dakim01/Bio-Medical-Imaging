{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 1267593,
     "sourceType": "datasetVersion",
     "datasetId": 723383
    },
    {
     "sourceId": 1299795,
     "sourceType": "datasetVersion",
     "datasetId": 751906
    },
    {
     "sourceId": 2006709,
     "sourceType": "datasetVersion",
     "datasetId": 1200659
    }
   ],
   "dockerImageVersionId": 30746,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Brain Tumor Segmentation with U-Net\n\nThis notebook trains a 2D U-Net to segment brain tumors from multimodal MRI scans using the **BraTS 2020** dataset. Each patient has four MRI modalities (T1, T1ce, T2, FLAIR) paired with expert-annotated segmentation masks identifying three tumour sub-regions:\n\n| Label | Class | Description |\n|-------|-------|-------------|\n| 0 | Background | Healthy brain tissue |\n| 1 | Necrotic core | Dead or non-enhancing tumour core |\n| 2 | Edema | Peritumoral swelling |\n| 4\u21923 | Enhancing tumour | Gadolinium-enhancing active tumour |\n\n> **Label 3 is absent** from the dataset; we remap label 4 \u2192 3 for contiguous one-hot encoding.\n\nWe use only **FLAIR + T1ce** \u2014 two complementary modalities that maximise tumour contrast while minimising computational cost."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. Environment Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install Kaggle API (only needed once in Colab/Kaggle)\n!pip install kaggle -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Authenticate and download the BraTS 2020 dataset\nimport os, shutil\n\nos.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\nshutil.copy('kaggle.json', os.path.expanduser('~/.kaggle/kaggle.json'))\nos.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 0o600)\n\n!kaggle datasets download awsaf49/brats20-dataset-training-validation -q\n!unzip -q brats20-dataset-training-validation.zip"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Imports & Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Standard library \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport os\nimport glob\nimport random\n\n# \u2500\u2500 Scientific computing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport numpy as np\nimport pandas as pd\n\n# \u2500\u2500 Image processing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport cv2\nimport nibabel as nib\nfrom skimage.util import montage\nfrom skimage.transform import rotate\n\n# \u2500\u2500 Visualisation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# \u2500\u2500 Machine learning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# \u2500\u2500 Deep learning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport tensorflow as tf\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import CSVLogger\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, MaxPooling2D, UpSampling2D,\n    concatenate, Dropout\n)\nfrom tensorflow.keras.utils import plot_model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Global constants \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTRAIN_DATASET_PATH = '/content/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n\nIMG_SIZE       = 128   # Resize each 2D slice to this resolution\nVOLUME_SLICES  = 100   # Number of axial slices to use per patient\nVOLUME_START   = 22    # First slice index (skip uninformative edge slices)\nN_CLASSES      = 4     # Background, Necrotic, Edema, Enhancing\nBATCH_SIZE     = 1     # Patients per batch\n\nSEGMENT_CLASSES = {\n    0: 'Background',\n    1: 'Necrotic / Core',\n    2: 'Edema',\n    3: 'Enhancing tumour',  # original label 4, remapped to 3\n}\n\n# Colourmap for 4-class segmentation masks\nSEG_CMAP = matplotlib.colors.ListedColormap(['#440054', '#3b528b', '#18b880', '#e6d74f'])\nSEG_NORM = matplotlib.colors.BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], SEG_CMAP.N)\n\nscaler = MinMaxScaler()\nprint('\u2713 Configuration loaded')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Fix Malformed File in the Dataset\n\nOne patient folder (`BraTS20_Training_355`) contains an incorrectly named segmentation file. We rename it to match the expected naming convention before proceeding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "old_name = os.path.join(TRAIN_DATASET_PATH, 'BraTS20_Training_355', 'W39_1998.09.19_Segm.nii')\nnew_name = os.path.join(TRAIN_DATASET_PATH, 'BraTS20_Training_355', 'BraTS20_Training_355_seg.nii')\n\ntry:\n    os.rename(old_name, new_name)\n    print('\u2713 Segmentation file renamed successfully.')\nexcept FileNotFoundError:\n    print('\u2713 File already renamed \u2014 nothing to do.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Explore the Dataset\n\n### 4.1 Load a Sample Patient"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_and_scale(filepath):\n    \"\"\"Load a NIfTI volume and min-max scale it to [0, 1].\"\"\"\n    vol = nib.load(filepath).get_fdata()\n    return scaler.fit_transform(vol.reshape(-1, vol.shape[-1])).reshape(vol.shape)\n\nsample_id   = 'BraTS20_Training_355'\nsample_dir  = os.path.join(TRAIN_DATASET_PATH, sample_id)\n\nflair_sample = load_and_scale(os.path.join(sample_dir, f'{sample_id}_flair.nii'))\nt1_sample    = load_and_scale(os.path.join(sample_dir, f'{sample_id}_t1.nii'))\nt1ce_sample  = load_and_scale(os.path.join(sample_dir, f'{sample_id}_t1ce.nii'))\nt2_sample    = load_and_scale(os.path.join(sample_dir, f'{sample_id}_t2.nii'))\nseg_sample   = nib.load(os.path.join(sample_dir, f'{sample_id}_seg.nii')).get_fdata()\n\nprint(f'Modality shape : {flair_sample.shape}')\nprint(f'Mask shape     : {seg_sample.shape}')\nprint(f'Intensity range after scaling: [{flair_sample.min():.3f}, {flair_sample.max():.3f}]')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Visualise All Four Modalities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "EXAMPLE_SLICE = 95\n\nfig, axes = plt.subplots(2, 3, figsize=(14, 9))\nmodalities = [\n    (t1_sample,    'T1',   'gray'),\n    (t1ce_sample,  'T1ce', 'gray'),\n    (t2_sample,    'T2',   'gray'),\n    (flair_sample, 'FLAIR','gray'),\n    (seg_sample,   'Mask', None),\n]\n\nfor ax, (vol, title, cmap) in zip(axes.flat, modalities):\n    kwargs = dict(cmap=SEG_CMAP, norm=SEG_NORM) if cmap is None else dict(cmap=cmap)\n    ax.imshow(vol[:, :, EXAMPLE_SLICE], **kwargs)\n    ax.set_title(title, fontsize=13, fontweight='bold')\n    ax.axis('off')\n\naxes.flat[-1].set_visible(False)  # hide empty subplot\nplt.suptitle(f'Sample patient \u2014 axial slice {EXAMPLE_SLICE}', fontsize=15)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 Three Anatomical Planes\n\nEach 3D volume can be sliced along three orthogonal planes: **axial** (top-down), **coronal** (front-back) and **sagittal** (left-right). We apply a 90\u00b0 rotation so all views render upright."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\nviews = [\n    (t1ce_sample[:, :, EXAMPLE_SLICE],          'Axial'),\n    (rotate(t1ce_sample[:, EXAMPLE_SLICE, :], 90, resize=True), 'Coronal'),\n    (rotate(t1ce_sample[EXAMPLE_SLICE, :, :], 90, resize=True), 'Sagittal'),\n]\n\nfor ax, (img, title) in zip(axes, views):\n    ax.imshow(img, cmap='gray')\n    ax.set_title(title, fontsize=13, fontweight='bold')\n    ax.axis('off')\n\nplt.suptitle('T1ce \u2014 three anatomical planes', fontsize=15)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.4 Montage: All Axial Slices\n\nMany edge slices are empty (pure black). We visualise all slices and then restrict to a useful window (50 : -50)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n\naxes[0].imshow(rotate(montage(t1ce_sample[:, :, :]), 90, resize=True), cmap='gray')\naxes[0].set_title('All slices', fontsize=13)\naxes[0].axis('off')\n\naxes[1].imshow(rotate(montage(t1ce_sample[50:-50, :, :]), 90, resize=True), cmap='gray')\naxes[1].set_title('Slices 50 : -50 (informative range)', fontsize=13)\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.5 Segmentation Class Breakdown"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Isolate each class by masking non-matching labels to NaN\ndef isolate_class(seg, label):\n    masked = seg.copy().astype(float)\n    masked[masked != label] = np.nan\n    return masked\n\nfig, axes = plt.subplots(1, 5, figsize=(22, 5))\nclass_labels = [0, 1, 2, 4]\nclass_titles = ['Original', 'Background (0)', 'Necrotic (1)', 'Edema (2)', 'Enhancing (4)']\n\naxes[0].imshow(seg_sample[:, :, EXAMPLE_SLICE], cmap=SEG_CMAP, norm=SEG_NORM)\naxes[0].set_title('Original', fontweight='bold')\n\nlegend_patches = [\n    plt.Rectangle((0,0), 1, 1, color=SEG_CMAP(i), label=f'Class {class_labels[i]}')\n    for i in range(len(class_labels))\n]\naxes[0].legend(handles=legend_patches, loc='lower left', fontsize=8)\n\nfor ax, label, title in zip(axes[1:], class_labels, class_titles[1:]):\n    ax.imshow(isolate_class(seg_sample, label)[:, :, EXAMPLE_SLICE], cmap=SEG_CMAP, norm=SEG_NORM)\n    ax.set_title(title, fontweight='bold')\n\nfor ax in axes:\n    ax.axis('off')\n\nplt.suptitle('Segmentation class isolation', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Split the Dataset\n\nWe split 369 patient IDs into **train / validation / test** sets (\u224868 / 20 / 12 %) using stratified random sampling with a fixed seed for reproducibility."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "RANDOM_SEED = 42\n\nall_dirs = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\nall_ids  = [d.split('/')[-1] for d in all_dirs]\n\ntrain_val_ids, test_ids  = train_test_split(all_ids,  test_size=0.12, random_state=RANDOM_SEED)\ntrain_ids,     val_ids   = train_test_split(train_val_ids, test_size=0.19, random_state=RANDOM_SEED)\n\nprint(f'Train      : {len(train_ids):>3} patients')\nprint(f'Validation : {len(val_ids):>3} patients')\nprint(f'Test       : {len(test_ids):>3} patients')\nprint(f'Total      : {len(all_ids):>3} patients')\n\n# Bar chart\nfig, ax = plt.subplots(figsize=(6, 4))\nax.bar(['Train', 'Validation', 'Test'],\n       [len(train_ids), len(val_ids), len(test_ids)],\n       color=['#2ecc71', '#e74c3c', '#3498db'])\nax.set_ylabel('Patients')\nax.set_title('Dataset split')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Data Generator\n\nThe `BraTSDataGenerator` streams preprocessed 2D slices directly from disk, avoiding out-of-memory issues with the full 3D volumes.\n\n**Per batch, for each patient:**\n1. Load FLAIR and T1ce volumes.\n2. Extract `VOLUME_SLICES` axial slices starting at `VOLUME_START`.\n3. Resize each slice to `IMG_SIZE \u00d7 IMG_SIZE`.\n4. Remap label 4 \u2192 3 (no label 3 exists in BraTS 2020).\n5. One-hot encode the mask into `N_CLASSES` channels.\n6. Normalise X to `[0, 1]`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class BraTSDataGenerator(keras.utils.Sequence):\n    \"\"\"Keras-compatible generator for the BraTS 2020 dataset.\n\n    Yields batches of (X, Y) where:\n        X : float32 array of shape (VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2)\n            Channel 0 = FLAIR, Channel 1 = T1ce\n        Y : float32 one-hot array of shape (VOLUME_SLICES, IMG_SIZE, IMG_SIZE, N_CLASSES)\n    \"\"\"\n\n    def __init__(self, patient_ids, img_size=IMG_SIZE, batch_size=BATCH_SIZE,\n                 n_channels=2, shuffle=True):\n        self.patient_ids = patient_ids\n        self.img_size    = img_size\n        self.batch_size  = batch_size\n        self.n_channels  = n_channels\n        self.shuffle     = shuffle\n        self.on_epoch_end()\n\n    # \u2500\u2500 Keras Sequence API \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    def __len__(self):\n        \"\"\"Number of batches per epoch.\"\"\"\n        return len(self.patient_ids) // self.batch_size\n\n    def __getitem__(self, index):\n        \"\"\"Return one batch of (X, Y).\"\"\"\n        batch_ids = [\n            self.patient_ids[k]\n            for k in self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        ]\n        return self._load_batch(batch_ids)\n\n    def on_epoch_end(self):\n        \"\"\"Shuffle patient order after every epoch (if enabled).\"\"\"\n        self.indexes = np.arange(len(self.patient_ids))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    # \u2500\u2500 Internal helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    def _load_volume(self, case_dir, case_id, modality):\n        \"\"\"Load a single NIfTI modality volume as a numpy array.\"\"\"\n        path = os.path.join(case_dir, f'{case_id}_{modality}.nii')\n        return nib.load(path).get_fdata()\n\n    def _load_batch(self, batch_ids):\n        \"\"\"Load and preprocess all patients in a batch.\"\"\"\n        n_slices = self.batch_size * VOLUME_SLICES\n        dim      = (self.img_size, self.img_size)\n\n        X = np.zeros((n_slices, *dim, self.n_channels), dtype=np.float32)\n        y = np.zeros((n_slices, 240, 240),               dtype=np.float32)\n\n        for patient_idx, patient_id in enumerate(batch_ids):\n            case_dir = os.path.join(TRAIN_DATASET_PATH, patient_id)\n\n            flair = self._load_volume(case_dir, patient_id, 'flair')\n            t1ce  = self._load_volume(case_dir, patient_id, 't1ce')\n            seg   = self._load_volume(case_dir, patient_id, 'seg')\n\n            slice_offset = patient_idx * VOLUME_SLICES\n            for j in range(VOLUME_SLICES):\n                z = j + VOLUME_START\n                X[slice_offset + j, :, :, 0] = cv2.resize(flair[:, :, z], dim)\n                X[slice_offset + j, :, :, 1] = cv2.resize(t1ce[:, :, z], dim)\n                y[slice_offset + j]           = seg[:, :, z]\n\n        # Remap label 4 \u2192 3 (label 3 is absent in BraTS 2020)\n        y[y == 4] = 3\n\n        # One-hot encode and resize masks\n        Y = tf.image.resize(tf.one_hot(y.astype(np.int32), N_CLASSES), dim)\n\n        # Normalise images to [0, 1]\n        x_max = X.max()\n        return (X / x_max if x_max > 0 else X), Y\n\n\n# Instantiate generators\ntraining_generator   = BraTSDataGenerator(train_ids, shuffle=True)\nvalidation_generator = BraTSDataGenerator(val_ids,   shuffle=False)\ntest_generator       = BraTSDataGenerator(test_ids,  shuffle=False)\n\nprint(f'Training batches   : {len(training_generator)}')\nprint(f'Validation batches : {len(validation_generator)}')\nprint(f'Test batches       : {len(test_generator)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Sanity check \u2014 visualise a batch sample"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_slice_triplet(flair_slice, t1ce_slice, mask_slice, title=''):\n    \"\"\"Display FLAIR, T1ce and the segmentation mask side by side.\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    axes[0].imshow(flair_slice, cmap='gray');  axes[0].set_title('FLAIR')\n    axes[1].imshow(t1ce_slice,  cmap='gray');  axes[1].set_title('T1ce')\n    axes[2].imshow(mask_slice,  cmap=SEG_CMAP, norm=SEG_NORM)\n    axes[2].set_title('Segmentation mask')\n    for ax in axes:\n        ax.axis('off')\n    if title:\n        plt.suptitle(title, fontsize=13)\n    plt.tight_layout()\n    plt.show()\n\n# Pull a batch and display one slice\nX_batch, Y_batch = training_generator[8]\nmasks = np.argmax(Y_batch, axis=-1)   # one-hot \u2192 class index\n\nDISPLAY_SLICE = 60\nplot_slice_triplet(\n    X_batch[DISPLAY_SLICE, :, :, 0],\n    X_batch[DISPLAY_SLICE, :, :, 1],\n    masks[DISPLAY_SLICE],\n    title=f'Training batch 8, slice {DISPLAY_SLICE}',\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Loss Function & Evaluation Metrics\n\nWe combine **categorical cross-entropy** (pixel-level class probabilities) with **Dice loss** (overlap-based). Additional per-class Dice scores track performance on each tumour sub-region separately."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Dice coefficient (mean across all 4 classes) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    \"\"\"Macro-averaged Dice coefficient over all N_CLASSES channels.\"\"\"\n    total = 0.0\n    for i in range(N_CLASSES):\n        y_t = K.flatten(y_true[:, :, :, i])\n        y_p = K.flatten(y_pred[:, :, :, i])\n        intersection = K.sum(y_t * y_p)\n        total += (2.0 * intersection + smooth) / (K.sum(y_t) + K.sum(y_p) + smooth)\n    return total / N_CLASSES\n\n\n# \u2500\u2500 Per-class Dice helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef _dice_for_class(y_true, y_pred, class_idx, epsilon=1e-6):\n    \"\"\"Dice coefficient for a single segmentation class.\"\"\"\n    y_t = y_true[:, :, :, class_idx]\n    y_p = y_pred[:, :, :, class_idx]\n    intersection = K.sum(K.abs(y_t * y_p))\n    return (2.0 * intersection) / (K.sum(K.square(y_t)) + K.sum(K.square(y_p)) + epsilon)\n\ndef dice_coef_necrotic(y_true, y_pred):\n    return _dice_for_class(y_true, y_pred, class_idx=1)\n\ndef dice_coef_edema(y_true, y_pred):\n    return _dice_for_class(y_true, y_pred, class_idx=2)\n\ndef dice_coef_enhancing(y_true, y_pred):\n    return _dice_for_class(y_true, y_pred, class_idx=3)\n\n\n# \u2500\u2500 Classification metrics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef precision(y_true, y_pred):\n    \"\"\"Positive predictive value.\"\"\"\n    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    pp = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    return tp / (pp + K.epsilon())\n\ndef sensitivity(y_true, y_pred):\n    \"\"\"True positive rate (recall).\"\"\"\n    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    ap = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return tp / (ap + K.epsilon())\n\ndef specificity(y_true, y_pred):\n    \"\"\"True negative rate.\"\"\"\n    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    an = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n    return tn / (an + K.epsilon())\n\n\nALL_METRICS = [\n    'accuracy',\n    tf.keras.metrics.MeanIoU(num_classes=N_CLASSES),\n    dice_coef, precision, sensitivity, specificity,\n    dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing,\n]\nprint('\u2713 Metrics defined')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8. U-Net Architecture\n\nOur 2D U-Net follows the classic encoder \u2192 bottleneck \u2192 decoder design with skip connections. Each encoder stage halves spatial resolution while doubling filter count; the decoder mirrors this in reverse.\n\n| Stage | Filters | Spatial size |\n|-------|---------|-------------|\n| Encoder 1 | 32 | 128 \u00d7 128 |\n| Encoder 2 | 64 | 64 \u00d7 64 |\n| Encoder 3 | 128 | 32 \u00d7 32 |\n| Encoder 4 | 256 | 16 \u00d7 16 |\n| Bottleneck | 512 | 8 \u00d7 8 |\n| Decoder \u2192 Output | 256 \u2192 32 | 128 \u00d7 128 |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def conv_block(x, filters, kernel_init):\n    \"\"\"Two consecutive Conv2D-ReLU layers (the core U-Net building block).\"\"\"\n    x = Conv2D(filters, 3, activation='relu', padding='same', kernel_initializer=kernel_init)(x)\n    x = Conv2D(filters, 3, activation='relu', padding='same', kernel_initializer=kernel_init)(x)\n    return x\n\ndef upsample_block(x, filters, kernel_init):\n    \"\"\"Upsample by 2x then refine with a single Conv2D.\"\"\"\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(filters, 2, activation='relu', padding='same', kernel_initializer=kernel_init)(x)\n    return x\n\n\ndef build_unet(input_shape=(IMG_SIZE, IMG_SIZE, 2),\n               kernel_init='he_normal',\n               dropout_rate=0.2):\n    \"\"\"Build and return a 2D U-Net model.\n\n    Args:\n        input_shape  : (height, width, channels) \u2014 default (128, 128, 2).\n        kernel_init  : Weight initialiser for Conv2D layers.\n        dropout_rate : Dropout probability applied at the bottleneck.\n\n    Returns:\n        A compiled-ready Keras Model.\n    \"\"\"\n    inputs = Input(input_shape)\n\n    # \u2500\u2500 Encoder \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    enc1 = conv_block(inputs, 32,  kernel_init)\n    enc2 = conv_block(MaxPooling2D()(enc1), 64,  kernel_init)\n    enc3 = conv_block(MaxPooling2D()(enc2), 128, kernel_init)\n    enc4 = conv_block(MaxPooling2D()(enc3), 256, kernel_init)\n\n    # \u2500\u2500 Bottleneck \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    bridge = conv_block(MaxPooling2D()(enc4), 512, kernel_init)\n    bridge = Dropout(dropout_rate)(bridge)\n\n    # \u2500\u2500 Decoder \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    dec4 = conv_block(concatenate([upsample_block(bridge, 256, kernel_init), enc4]), 256, kernel_init)\n    dec3 = conv_block(concatenate([upsample_block(dec4,   128, kernel_init), enc3]), 128, kernel_init)\n    dec2 = conv_block(concatenate([upsample_block(dec3,    64, kernel_init), enc2]),  64, kernel_init)\n    dec1 = conv_block(concatenate([upsample_block(dec2,    32, kernel_init), enc1]),  32, kernel_init)\n\n    # \u2500\u2500 Output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    outputs = Conv2D(N_CLASSES, (1, 1), activation='softmax')(dec1)\n\n    return Model(inputs=inputs, outputs=outputs, name='UNet_BraTS')\n\n\nmodel = build_unet()\nmodel.summary(line_length=90)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plot_model(model, show_shapes=True, show_layer_names=True,\n           rankdir='TB', expand_nested=False, dpi=70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 9. Callbacks & Training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "callbacks = [\n    keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', factor=0.2, patience=2,\n        min_lr=1e-6, verbose=1,\n    ),\n    keras.callbacks.ModelCheckpoint(\n        filepath='model_epoch{epoch:02d}_valloss{val_loss:.6f}.weights.h5',\n        monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1,\n    ),\n    CSVLogger('training.log', separator=',', append=False),\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n    loss='categorical_crossentropy',\n    metrics=ALL_METRICS,\n)\n\nK.clear_session()\n\nhistory = model.fit(\n    training_generator,\n    epochs=35,\n    steps_per_epoch=len(train_ids),\n    validation_data=validation_generator,\n    callbacks=callbacks,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model.save('brain_tumor_unet.keras')\nprint('\u2713 Model saved.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 10. Load a Saved Model\n\nUse this cell to reload the final model or a specific checkpoint (e.g. the best epoch by validation loss)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CUSTOM_OBJECTS = {\n    'dice_coef'          : dice_coef,\n    'precision'          : precision,\n    'sensitivity'        : sensitivity,\n    'specificity'        : specificity,\n    'dice_coef_necrotic' : dice_coef_necrotic,\n    'dice_coef_edema'    : dice_coef_edema,\n    'dice_coef_enhancing': dice_coef_enhancing,\n}\n\n# \u2500\u2500 Option A: load the full saved model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nmodel = keras.models.load_model('brain_tumor_unet.keras',\n                                custom_objects=CUSTOM_OBJECTS,\n                                compile=False)\n\n# \u2500\u2500 Option B: load best checkpoint weights \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# model = build_unet()\n# model.load_weights('model_epoch19_valloss0.021449.weights.h5')\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n    loss='categorical_crossentropy',\n    metrics=ALL_METRICS,\n)\nprint('\u2713 Model loaded and compiled.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 11. Training Curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "log = pd.read_csv('training.log')\n\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\nmetric_pairs = [\n    ('accuracy',  'val_accuracy',  'Accuracy'),\n    ('loss',      'val_loss',      'Loss'),\n    ('dice_coef', 'val_dice_coef', 'Dice Coefficient'),\n    ('mean_io_u', 'val_mean_io_u', 'Mean IoU'),\n]\n\nepochs = range(1, len(log) + 1)\n\nfor ax, (train_col, val_col, title) in zip(axes, metric_pairs):\n    ax.plot(epochs, log[train_col], 'b-o', markersize=3, label='Train')\n    ax.plot(epochs, log[val_col],   'r-o', markersize=3, label='Validation')\n    ax.set_title(title, fontsize=12, fontweight='bold')\n    ax.set_xlabel('Epoch')\n    ax.legend()\n    ax.grid(alpha=0.3)\n\nplt.suptitle('Training history', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 12. Predictions on the Test Set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def predict_patient(patient_id):\n    \"\"\"Run model inference on all VOLUME_SLICES axial slices for one patient.\n\n    Returns:\n        p : float32 array of shape (VOLUME_SLICES, IMG_SIZE, IMG_SIZE, N_CLASSES)\n            Softmax probabilities for every class.\n    \"\"\"\n    case_dir = os.path.join(TRAIN_DATASET_PATH, patient_id)\n    dim = (IMG_SIZE, IMG_SIZE)\n    X = np.zeros((VOLUME_SLICES, *dim, 2), dtype=np.float32)\n\n    flair = nib.load(os.path.join(case_dir, f'{patient_id}_flair.nii')).get_fdata()\n    t1ce  = nib.load(os.path.join(case_dir, f'{patient_id}_t1ce.nii')).get_fdata()\n\n    for j in range(VOLUME_SLICES):\n        z = j + VOLUME_START\n        X[j, :, :, 0] = cv2.resize(flair[:, :, z], dim)\n        X[j, :, :, 1] = cv2.resize(t1ce[:, :, z],  dim)\n\n    x_max = X.max()\n    return model.predict(X / x_max if x_max > 0 else X, verbose=0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def show_prediction(patient_id, slice_idx=60):\n    \"\"\"Overlay predicted tumour classes on the FLAIR background for one slice.\n\n    Args:\n        patient_id : BraTS patient ID string (e.g. 'BraTS20_Training_042').\n        slice_idx  : Index into the VOLUME_SLICES window to display.\n    \"\"\"\n    case_dir = os.path.join(TRAIN_DATASET_PATH, patient_id)\n    dim = (IMG_SIZE, IMG_SIZE)\n\n    # Load ground truth and background image\n    flair_vol = nib.load(os.path.join(case_dir, f'{patient_id}_flair.nii')).get_fdata()\n    gt_vol    = nib.load(os.path.join(case_dir, f'{patient_id}_seg.nii')).get_fdata()\n\n    bg  = cv2.resize(flair_vol[:, :, slice_idx + VOLUME_START], dim)\n    gt  = cv2.resize(gt_vol[:,   :, slice_idx + VOLUME_START], dim, interpolation=cv2.INTER_NEAREST)\n\n    p = predict_patient(patient_id)\n\n    fig, axes = plt.subplots(1, 6, figsize=(22, 5))\n\n    panels = [\n        (bg,                            None,   None,    'FLAIR (original)'),\n        (gt,                            SEG_CMAP, SEG_NORM, 'Ground truth'),\n        (p[slice_idx, :, :, 1:4],       'Reds', None,    'All tumour classes'),\n        (p[slice_idx, :, :, 1],         'OrRd', None,    f'{SEGMENT_CLASSES[1]} (pred)'),\n        (p[slice_idx, :, :, 2],         'OrRd', None,    f'{SEGMENT_CLASSES[2]} (pred)'),\n        (p[slice_idx, :, :, 3],         'OrRd', None,    f'{SEGMENT_CLASSES[3]} (pred)'),\n    ]\n\n    for ax, (img, cmap, norm, title) in zip(axes, panels):\n        # Show FLAIR background first, then overlay prediction with transparency\n        if cmap is not None and 'Reds' in str(cmap) or cmap == 'OrRd':\n            ax.imshow(bg, cmap='gray')\n            ax.imshow(img, cmap=cmap, alpha=0.4, interpolation='none')\n        elif norm is not None:\n            ax.imshow(bg, cmap='gray')\n            ax.imshow(img, cmap=cmap, norm=norm, alpha=0.4, interpolation='none')\n        else:\n            ax.imshow(img, cmap='gray')\n        ax.set_title(title, fontsize=10, fontweight='bold')\n        ax.axis('off')\n\n    plt.suptitle(f'Patient {patient_id} \u2014 slice {slice_idx}', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\n\n# Display predictions for the first 7 test patients\nfor pid in test_ids[:7]:\n    show_prediction(pid, slice_idx=60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 13. Final Evaluation on the Test Set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = model.evaluate(test_generator, batch_size=100, verbose=1)\n\nmetric_names = [\n    'Loss', 'Accuracy', 'Mean IoU', 'Dice Coefficient',\n    'Precision', 'Sensitivity', 'Specificity',\n    'Dice \u2014 Necrotic', 'Dice \u2014 Edema', 'Dice \u2014 Enhancing',\n]\n\nprint('\\n' + '='*45)\nprint('  Model evaluation \u2014 test set')\nprint('='*45)\nfor name, value in zip(metric_names, results):\n    print(f'  {name:<22} : {value:.4f}')\nprint('='*45)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 14. Conclusion\n\nThis notebook walked through the full pipeline for brain tumour segmentation with U-Net on BraTS 2020:\n\n- **Data exploration** \u2014 understood the four MRI modalities and three anatomical planes.\n- **Preprocessing** \u2014 normalised intensities, selected informative axial slices, and one-hot encoded masks.\n- **Data generator** \u2014 streamed batches from disk to avoid memory overflow.\n- **U-Net** \u2014 implemented a modular encoder-decoder with skip connections.\n- **Training** \u2014 used adaptive learning rate, best-model checkpointing, and CSV logging.\n- **Evaluation** \u2014 measured Dice, IoU, sensitivity, and specificity on a held-out test set.\n\n**Potential next steps:**\n- Try a 3D U-Net to exploit inter-slice spatial context.\n- Add data augmentation (flips, rotations, intensity jitter).\n- Experiment with combined Dice + cross-entropy loss weighting.\n- Use all four modalities instead of two."
  }
 ]
}