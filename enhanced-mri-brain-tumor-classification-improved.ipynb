{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "kaggle": {
   "accelerator": "gpu",
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# \ud83d\udd0d Enhanced MRI Brain Tumor Classification\n\nThis notebook builds a ResNet50V2-based classifier that distinguishes between four brain MRI categories:\n**Glioma \u00b7 Meningioma \u00b7 Pituitary Tumor \u00b7 Normal**\n\nA key design goal is robustness on *low-quality* images, simulating conditions in under-resourced healthcare settings.\n\n---\n## Table of Contents\n1. [Setup & Imports](#1-setup--imports)\n2. [Dataset Analysis](#2-dataset-analysis)\n3. [Dataset Preparation](#3-dataset-preparation)\n   - 3.1 [Build Image DataFrame](#31-build-image-dataframe)\n   - 3.2 [Train / Validation Split](#32-train--validation-split)\n   - 3.3 [Image Degradation Augmentation](#33-image-degradation-augmentation)\n   - 3.4 [Data Generators](#34-data-generators)\n4. [Model Architecture](#4-model-architecture)\n5. [Training & Fine-Tuning](#5-training--fine-tuning)\n6. [Performance Assessment](#6-performance-assessment)\n   - 6.1 [Learning Curves](#61-learning-curves)\n   - 6.2 [Classification Report & Confusion Matrix](#62-classification-report--confusion-matrix)\n7. [Save Model for Deployment](#7-save-model-for-deployment)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1  Setup & Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings('ignore')\n\n# Standard library\nimport os\nimport random\nimport subprocess\n\n# Third-party\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# TensorFlow / Keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.resnet_v2 import preprocess_input\nfrom tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.utils import plot_model\n\n# IPython helpers\nfrom IPython.display import FileLink, display\n\n# Reproducibility\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\n# Plot styling\nsns.set(rc={'axes.facecolor': '#e9eef2'}, style='darkgrid')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2  Dataset Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Copy dataset from Kaggle input to working directory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n!cp -r /kaggle/input/mri-images/Data /kaggle/working/\n\nBASE_DIR = '/kaggle/working/Data'\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1  Class distribution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Collect class names and image counts\nclasses = [\n    name for name in os.listdir(BASE_DIR)\n    if os.path.isdir(os.path.join(BASE_DIR, name))\n]\n\ncounts = [\n    len(os.listdir(os.path.join(BASE_DIR, cls)))\n    for cls in classes\n]\n\ntotal = sum(counts)\npercentages = [c / total * 100 for c in counts]\n\n# Plot\nfig, ax = plt.subplots(figsize=(14, 4))\nsns.barplot(y=classes, x=counts, orient='h', color='#102C42', ax=ax)\n\nax.set_xticks(range(0, max(counts) + 300, 100))\nax.set_xlabel('Number of Images', fontsize=13)\nax.set_title('Images per Class', fontsize=16)\n\nfor i, patch in enumerate(ax.patches):\n    ax.text(\n        patch.get_width() + 5,\n        patch.get_y() + patch.get_height() / 2,\n        f'{percentages[i]:.1f}%  ({counts[i]})',\n        va='center', fontsize=13\n    )\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total images: {total}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Note:** The `normal` class is the smallest \u2014 worth monitoring for class imbalance during training."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2  Image dimensions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "heights, widths = [], []\nunique_dims = set()\n\nfor cls in classes:\n    folder = os.path.join(BASE_DIR, cls)\n    for fname in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, fname))\n        if img is not None:\n            h, w = img.shape[:2]\n            unique_dims.add((h, w))\n            heights.append(h)\n            widths.append(w)\n\nif len(unique_dims) == 1:\n    print(f\"All images share the same dimensions: {list(unique_dims)[0]}\")\nelse:\n    print(f\"{len(unique_dims)} unique dimension(s) found.\")\n    print(f\"Height \u2014 min: {min(heights)}, max: {max(heights)}, mean: {np.mean(heights):.1f}\")\n    print(f\"Width  \u2014 min: {min(widths)},  max: {max(widths)},  mean: {np.mean(widths):.1f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3  Visual sample per class"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_image_grid(image_paths, title, n_cols=6):\n    \"\"\"Display a row of images with a shared title.\"\"\"\n    fig, axes = plt.subplots(1, n_cols, figsize=(15, 3))\n    for ax, path in zip(axes, image_paths):\n        img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n        ax.imshow(img)\n        ax.axis('off')\n    fig.suptitle(title, fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n\nfor cls in classes:\n    folder = os.path.join(BASE_DIR, cls)\n    all_paths = [os.path.join(folder, f) for f in os.listdir(folder)]\n    sample = np.random.choice(all_paths, 6, replace=False)\n    plot_image_grid(sample, f\"{cls}  \u2014  original samples\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3  Dataset Preparation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1  Build Image DataFrame"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Collect (filepath, label) pairs for every image in every class\nrecords = [\n    (os.path.join(BASE_DIR, cls, fname), cls)\n    for cls in classes\n    for fname in os.listdir(os.path.join(BASE_DIR, cls))\n    if os.path.isfile(os.path.join(BASE_DIR, cls, fname))\n]\n\ndf = pd.DataFrame(records, columns=['filepath', 'label'])\nprint(f\"Total images in DataFrame: {len(df)}\")\ndf.head()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2  Train / Validation Split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 80 / 20 stratified split so class proportions are preserved\ntrain_df, val_df = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df['label'],\n    random_state=SEED\n)\n\nprint(f\"Train size : {len(train_df)}\")\nprint(f\"Val size   : {len(val_df)}\")\n\ndel df  # free memory\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3  Image Degradation Augmentation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "To make the model robust to poor imaging conditions we synthetically degrade a copy of every\nimage using one or more of three techniques:\n\n| Technique | Simulates |\n|-----------|-----------|\n| **Gaussian noise** | sensor noise / imaging artefacts |\n| **Gaussian blur** | patient movement / focus issues |\n| **Downsample \u2192 upsample** | low-resolution scanners |\n\nThe degraded copies are merged back into the dataset *after* the train/val split to prevent data leakage.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Degradation helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef add_gaussian_noise(image, mean=0, std=0.05):\n    \"\"\"Add Gaussian noise; output is clipped to [0, 255].\"\"\"\n    noise = np.random.normal(mean, std, image.shape)\n    return np.clip(image + noise, 0, 255)\n\n\ndef apply_gaussian_blur(image, kernel_size=5):\n    \"\"\"Apply Gaussian blur with the given (square) kernel size.\"\"\"\n    return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n\n\ndef downsample_upsample(image, scale_percent=50):\n    \"\"\"Shrink the image then restore its original size, reducing apparent resolution.\"\"\"\n    h, w = image.shape[:2]\n    small_w = max(1, int(w * scale_percent / 100))\n    small_h = max(1, int(h * scale_percent / 100))\n    small = cv2.resize(image, (small_w, small_h), interpolation=cv2.INTER_AREA)\n    return cv2.resize(small, (w, h), interpolation=cv2.INTER_LINEAR)\n\n\nDEGRADATION_METHODS = {\n    'noise':      add_gaussian_noise,\n    'blur':       apply_gaussian_blur,\n    'downsample': downsample_upsample,\n}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def augment_with_degraded_copies(dataframe):\n    \"\"\"\n    For every image in `dataframe`, create a degraded copy using a random\n    subset of degradation methods, save it alongside the original, and\n    return a new DataFrame that includes both the originals and the copies.\n    \n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        Must have columns ['filepath', 'label'].\n    \n    Returns\n    -------\n    pd.DataFrame\n        Combined original + degraded rows, index reset.\n    \"\"\"\n    degraded_records = []\n\n    for _, row in dataframe.iterrows():\n        img = cv2.imread(row['filepath'], cv2.IMREAD_GRAYSCALE)\n        if img is None:\n            continue  # skip unreadable files\n\n        # Pick 1\u20133 degradation methods at random\n        chosen = random.sample(\n            list(DEGRADATION_METHODS.keys()),\n            k=random.randint(1, len(DEGRADATION_METHODS))\n        )\n\n        for method_name in chosen:\n            img = DEGRADATION_METHODS[method_name](img)\n\n        # Save degraded image next to original with a prefix\n        directory, filename = os.path.split(row['filepath'])\n        degraded_path = os.path.join(directory, f\"degraded_{filename}\")\n        cv2.imwrite(degraded_path, img)\n\n        degraded_records.append({'filepath': degraded_path, 'label': row['label']})\n\n    degraded_df = pd.DataFrame(degraded_records)\n    return pd.concat([dataframe, degraded_df], ignore_index=True)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Before augmentation \u2014 train: {len(train_df)}, val: {len(val_df)}\")\n\ntrain_df = augment_with_degraded_copies(train_df)\nval_df   = augment_with_degraded_copies(val_df)\n\nprint(f\"After  augmentation \u2014 train: {len(train_df)}, val: {len(val_df)}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visual check: confirm tumour features are still visible after degradation\nfor cls in classes:\n    folder = os.path.join(BASE_DIR, cls)\n    all_paths = [os.path.join(folder, f) for f in os.listdir(folder)]\n    sample = np.random.choice(all_paths, 6, replace=False)\n    plot_image_grid(sample, f\"{cls}  \u2014  post-degradation samples\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4  Data Generators"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We use Keras's `ImageDataGenerator` for memory-efficient on-the-fly augmentation during training.\nThe **validation generator** intentionally skips geometric augmentation to give an unbiased estimate of performance.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Hyperparameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nIMAGE_SIZE  = (224, 224)   # ResNet50V2 expects 224\u00d7224\nBATCH_SIZE  = 32\n\n\ndef create_data_generators(train_df, val_df,\n                            preprocessing_fn=None,\n                            batch_size=BATCH_SIZE,\n                            image_size=IMAGE_SIZE):\n    \"\"\"\n    Build and return (train_generator, val_generator).\n\n    Training generator applies light geometric augmentation to improve\n    generalisation; validation generator only applies preprocessing.\n\n    Parameters\n    ----------\n    train_df : pd.DataFrame\n        Columns: ['filepath', 'label']\n    val_df : pd.DataFrame\n        Columns: ['filepath', 'label']\n    preprocessing_fn : callable, optional\n        Model-specific preprocessing (e.g. ResNet50V2's preprocess_input).\n    batch_size : int\n    image_size : tuple of (height, width)\n\n    Returns\n    -------\n    train_generator, val_generator\n    \"\"\"\n    shared_flow_kwargs = dict(\n        x_col='filepath',\n        y_col='label',\n        target_size=image_size,\n        batch_size=batch_size,\n        class_mode='categorical',\n        seed=SEED,\n    )\n\n    train_datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.10,\n        height_shift_range=0.10,\n        zoom_range=0.10,\n        horizontal_flip=True,\n        preprocessing_function=preprocessing_fn,\n    )\n\n    val_datagen = ImageDataGenerator(preprocessing_function=preprocessing_fn)\n\n    train_gen = train_datagen.flow_from_dataframe(\n        dataframe=train_df, shuffle=True, **shared_flow_kwargs\n    )\n    val_gen = val_datagen.flow_from_dataframe(\n        dataframe=val_df, shuffle=False, **shared_flow_kwargs\n    )\n\n    return train_gen, val_gen\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_generator, val_generator = create_data_generators(\n    train_df, val_df, preprocessing_fn=preprocess_input\n)\n\n# Sanity-check: confirm expected batch shape (batch_size \u00d7 224 \u00d7 224 \u00d7 3)\nsample_batch, _ = next(train_generator)\nprint(f\"Batch shape: {sample_batch.shape}\")\n\n# Class index mapping\nclass_names = sorted(train_generator.class_indices, key=train_generator.class_indices.get)\nprint(f\"Classes: {class_names}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Why 3-channel grayscale?**  MRI scans are naturally greyscale, but we load them as 3-channel images so they match the input format expected by ImageNet pre-trained models."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4  Model Architecture"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We use **ResNet50V2** pre-trained on ImageNet as a feature extractor, then attach a small\nclassification head. Transfer learning lets us achieve high accuracy despite a relatively small\ndataset while avoiding overfitting.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Base model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nbase_model = ResNet50V2(\n    weights='imagenet',\n    include_top=False,\n    input_shape=(*IMAGE_SIZE, 3)\n)\n\n# \u2500\u2500 Classification head \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.5)(x)                          # regularisation\noutput = Dense(4, activation='softmax')(x)   # 4 tumour classes\n\nmodel = Model(inputs=base_model.input, outputs=output)\n\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-4),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Architecture diagram (may be large \u2014 scroll down)\nplot_model(model, show_shapes=True, show_layer_names=False, dpi=150)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5  Training & Fine-Tuning"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Training hyperparameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nMAX_EPOCHS     = 50\nLR_PATIENCE    = 3    # epochs without val_loss improvement before halving LR\nEARLY_PATIENCE = 15   # epochs without improvement before stopping\n\n\ndef train_model(model, train_df, val_df,\n                preprocessing_fn,\n                image_size=IMAGE_SIZE,\n                batch_size=BATCH_SIZE,\n                max_epochs=MAX_EPOCHS):\n    \"\"\"\n    Train `model` and return the trained model, history, and val_generator.\n\n    Callbacks\n    ---------\n    ReduceLROnPlateau : halves the learning rate after `LR_PATIENCE` stagnant epochs.\n    EarlyStopping     : restores the best weights and stops after `EARLY_PATIENCE` epochs.\n    \"\"\"\n    train_gen, val_gen = create_data_generators(\n        train_df, val_df, preprocessing_fn, batch_size, image_size\n    )\n\n    callbacks = [\n        ReduceLROnPlateau(\n            monitor='val_loss', factor=0.5,\n            patience=LR_PATIENCE, min_lr=1e-5\n        ),\n        EarlyStopping(\n            monitor='val_loss', mode='min',\n            patience=EARLY_PATIENCE,\n            restore_best_weights=True, verbose=1\n        ),\n    ]\n\n    history = model.fit(\n        train_gen,\n        steps_per_epoch=len(train_gen),\n        epochs=max_epochs,\n        validation_data=val_gen,\n        validation_steps=len(val_gen),\n        callbacks=callbacks,\n    )\n\n    return model, history, val_gen\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model, history, val_generator = train_model(\n    model, train_df, val_df,\n    preprocessing_fn=preprocess_input\n)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6  Performance Assessment"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.1  Learning Curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_learning_curves(history, skip_first_n=5):\n    \"\"\"\n    Plot train / validation loss and accuracy over epochs.\n\n    Parameters\n    ----------\n    history : keras History object\n    skip_first_n : int\n        Skip the first N epochs to keep the y-axis scale readable\n        (early epochs can have very high loss).\n    \"\"\"\n    hist_df = pd.DataFrame(history.history).iloc[skip_first_n:]\n\n    sns.set(rc={'axes.facecolor': '#f0f0fc'}, style='darkgrid')\n    fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Loss\n    sns.lineplot(data=hist_df, x=hist_df.index, y='loss',\n                 color='#102C42', label='Train', ax=ax_loss)\n    sns.lineplot(data=hist_df, x=hist_df.index, y='val_loss',\n                 color='orangered', linestyle='--', label='Validation', ax=ax_loss)\n    ax_loss.set_title('Loss', fontsize=14)\n    ax_loss.set_xlabel('Epoch')\n\n    # Accuracy\n    sns.lineplot(data=hist_df, x=hist_df.index, y='accuracy',\n                 color='#102C42', label='Train', ax=ax_acc)\n    sns.lineplot(data=hist_df, x=hist_df.index, y='val_accuracy',\n                 color='orangered', linestyle='--', label='Validation', ax=ax_acc)\n    ax_acc.set_title('Accuracy', fontsize=14)\n    ax_acc.set_xlabel('Epoch')\n\n    plt.suptitle('Learning Curves', fontsize=16, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n\nplot_learning_curves(history)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.2  Classification Report & Confusion Matrix"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_model(model, val_generator):\n    \"\"\"\n    Print classification report and plot confusion matrix for the validation set.\n\n    Parameters\n    ----------\n    model : trained Keras model\n    val_generator : validation ImageDataGenerator flow\n    \"\"\"\n    class_labels = sorted(\n        val_generator.class_indices, key=val_generator.class_indices.get\n    )\n\n    # Predictions\n    raw_preds    = model.predict(val_generator, steps=len(val_generator))\n    pred_labels  = np.argmax(raw_preds, axis=1)\n    true_labels  = val_generator.classes\n\n    # Classification report\n    print(classification_report(true_labels, pred_labels, target_names=class_labels))\n\n    # Confusion matrix\n    cm = confusion_matrix(true_labels, pred_labels)\n    cmap = LinearSegmentedColormap.from_list('navy_white', ['white', '#102C42'])\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.heatmap(\n        cm, annot=True, fmt='d', cmap=cmap,\n        xticklabels=class_labels, yticklabels=class_labels,\n        ax=ax\n    )\n    ax.set_xlabel('Predicted Label', fontsize=12)\n    ax.set_ylabel('True Label', fontsize=12)\n    ax.set_title('Confusion Matrix', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n\nevaluate_model(model, val_generator)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7  Save Model for Deployment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MODEL_PATH = '/kaggle/working/ResNet50V2_brain_tumor.h5'\nmodel.save(MODEL_PATH)\nprint(f\"Model saved to {MODEL_PATH}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def zip_and_link(file_paths, archive_name):\n    \"\"\"\n    Zip `file_paths` into `archive_name`.zip inside /kaggle/working/\n    and display a download link.\n    \"\"\"\n    os.chdir('/kaggle/working/')\n    zip_path = f\"/kaggle/working/{archive_name}.zip\"\n    cmd = f\"zip {zip_path} \" + \" \".join(file_paths)\n\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Zip failed:\", result.stderr)\n        return\n\n    display(FileLink(f\"{archive_name}.zip\"))\n\n\nzip_and_link([MODEL_PATH], 'brain_tumor_model_archive')\n"
  }
 ]
}